# Tessera Known Traps

Tesseraプロジェクトで発見された技術的な罠（失敗パターン）を記録する。
将来の開発者（人間・AI）が同じ失敗を繰り返さないための知見集。

---

## TRAP-001: パス連打ハック（Pass Spamming Hack）

**発見日:** 2026-01-18
**深刻度:** 🔴 Critical
**状態:** 🔄 対処中

### 症状

- Loss が急激に改善（5.9 → 3.58）
- しかし Win Rate は 0%
- 対局を観察すると、パスを連打してゲームが即終了

### 原因

1. **パス制限なし**: 序盤からパスが可能
2. **終局即報酬**: パス連打で即座に勝敗が決まり、報酬を得られる
3. **サンプリング偏り**: 終局直前のサンプルに偏り、盤面中盤を学習しない

### なぜ危険か

- Loss という指標が「学習成功」を示さない
- 見かけ上の「相転移」が実は「ハックの学習」
- 長時間学習後に発覚すると、時間が無駄になる

### 対策

| 対策 | 説明 |
|------|------|
| **パス制限** | N手（例: 50手）まではパス禁止 |
| **8点サンプリング** | ゲーム全体から均等にサンプル |
| **パスペナルティ** | パス選択に対するペナルティ |
| **定期的なWin Rate確認** | Loss だけでなく eval_quick.py で挙動確認 |

### 教訓

> **「Loss の低下を無批判に喜ばない」**

Loss が改善しても、実際の性能（Win Rate）が伴わなければ意味がない。

---

## TRAP-002: vocab_size の次元混乱

**発見日:** 2026-01-19
**深刻度:** 🟡 Medium
**状態:** ✅ 解決

### 症状

- 文書によって vocab_size の記載が異なる（362, 363, 364）
- 実装と文書の乖離
- 新規参加者が混乱

### 原因

1. **設計変更の未反映**: EOS_TOKEN を廃止したが文書が古いまま
2. **複数の「次元」**: Embedding次元とPolicyHead出力次元が異なる
3. **コミット漏れ**: ローカル変更がGitHubに反映されていない

### 正しい理解

| 数値 | 意味 | 用途 |
|------|------|------|
| 361 | 盤上座標（19×19） | - |
| 362 | 盤上 + PASS | PolicyHead出力（着手可能空間） |
| 363 | 盤上 + PASS + PAD | vocab_size（Embedding次元） |
| 364 | 旧仕様（EOS含む） | **廃止** |

### 対策

1. **トークン設計セクションを全文書に追加**
2. **両方の次元を明記**（vocab_size と PolicyHead出力）
3. **Git同期の徹底**

### 教訓

> **「実装と文書を同時に更新する」**

---

## TRAP-003: MambaStateCapture によるメモリリーク

**発見日:** 2026-01-14（Phase II）
**深刻度:** 🔴 Critical
**状態:** ✅ 解決

### 症状

- ELO評価時にOOM（Out of Memory）が頻発
- 長時間学習で徐々にメモリ使用量が増加

### 原因

- forward hook が hidden state を保持し続けた
- ガベージコレクションが効かない

### 対策

**MambaStateCapture クラスを完全削除**

### 結果

790回のELO評価でOOMゼロ

### 教訓

> **「forward hook は危険。使う場合は必ずメモリ管理を確認」**

---

## TRAP-004: Pythonループによる速度低下

**発見日:** 2026-01-16（Phase III.1）
**深刻度:** 🟡 Medium
**状態:** ✅ 解決

### 症状

- 学習速度が 3.1 g/s で頭打ち
- GPU使用率が低い

### 原因

- 連検出・捕獲処理がPythonループで実装されていた
- CPUとGPU間の同期待ちが発生

### 対策

1. **Tromp-Taylor採用**: 捕獲ロジック自体を廃止
2. **全操作をテンソル演算に**: No Python Loop

### 結果

学習速度 3.1 → 15.6 g/s（5倍向上）

### 教訓

> **「GPU sovereignty を守れ。Pythonループは敵」**

---

## TRAP-005: 文書間の整合性崩壊

**発見日:** 2026-01-19
**深刻度:** 🟡 Medium
**状態:** ✅ 解決

### 症状

- README, HANDOFF, DESIGN_SPEC で記載が矛盾
- Phase III.2 が「完了」なのか「進行中」なのか不明
- どの文書が正しいのかわからない

### 原因

1. **更新漏れ**: 一部の文書だけ更新
2. **責務の曖昧さ**: どの文書に何を書くか不明確
3. **Git同期不足**: ローカル変更がpushされていない

### 対策

1. **文書責務の明確化**: DECISION_LOG, KNOWN_TRAPS 新設
2. **一括更新**: 関連文書を同時に更新
3. **Gitワークフロー徹底**: 変更したら即push

### 教訓

> **「文書は生き物。放置すると腐る」**

---

## TRAP-006: 「同意点即実行」の罠

**発見日:** 2026-01-19
**深刻度:** 🟡 Medium（LLM協業特有）
**状態:** ⚠️ 継続的注意

### 症状

- 複数のAI（Claude, Gemini, Copilot）が同意した内容を即実行
- 後から「実は見落としがあった」と判明

### 原因

- LLMは「同意」しやすい傾向がある
- 反論がないことは正しさの証明ではない
- 全員が同じ盲点を持っている可能性

### 対策

1. **「なぜ同意するのか」を問う**
2. **意図的に反論を求める**
3. **事実（データ、実験結果）で検証**
4. **Decision Log に記録し、後から検証可能に**

### 教訓

> **「同意は安心ではない。検証せよ」**

---

## テンプレート

新しい罠を記録する際は以下のフォーマットを使用：
```markdown
## TRAP-XXX: [罠の名前]

**発見日:** YYYY-MM-DD
**深刻度:** 🔴 Critical / 🟡 Medium / 🟢 Low
**状態:** ✅ 解決 / 🔄 対処中 / ⚠️ 継続的注意

### 症状
[どのような問題が発生したか]

### 原因
[なぜ発生したか]

### 対策
[どう解決したか / 解決するか]

### 教訓
> **「一言でまとめた教訓」**
```

---

*"Le symbole donne à penser."* — Paul Ricœur

## TRAP-007: 文書ベースの推測による実装状態の誤認

**発見日:** 2026-01-19
**深刻度:** 🟡 Medium
**状態:** ✅ 解決

### 症状

- ARCHITECTURE_OVERVIEW作成時、RayCast/Diffusion/Fusionを「将来構想（未実装）」と記載
- 実際にはソースコードに実装済み・有効化済みだった
- 文書（DESIGN_SPEC, HANDOFF等）に記載がなかったため、存在しないと判断

### 原因

1. **文書依存の調査**: LLMは文書を「真実のソース」として扱いがち
2. **ソースコード未確認**: 文書に書いてあることだけを信じた
3. **暗黙知の非文書化**: 開発者（山田さん）は知っていたが、LLMに伝える必要性を感じなかった

### 経緯

- 2025/1/14-15: Diffusion + RayCast + Fusion を実装（Phase III.1）
- 2025/1/17: Tromp-Taylor への方針転換、文書の焦点がそちらに移る
- 2026/1/19: ARCHITECTURE_OVERVIEW作成時に「未実装」と誤記
- 2026/1/19: ソースコード直接調査で実装済みと判明

### 対策

1. **ソースコード直接検査を必須化**: 文書だけでなく `grep` で実装確認
2. **「存在しない」と書く前に検証**: 特に複雑なコンポーネントについて
3. **実装ファイル一覧を文書に含める**: 何が存在するかを明示

### 教訓

> **「文書は不完全。ソースコードこそが真実のソース（Source of Truth）」**

---


## TRAP-008: 視点破綻（Perspective Collapse）

**発見日:** 2026-01-19
**深刻度:** 🔴 Critical
**状態:** ✅ 解決（DEC-008）

### 症状

- Loss が 5.9 で停滞し、相転移が起きない
- Win Rate が 0%（ランダムにすら勝てない）
- しかしパス連打ではなく、石は正常に配置されている
- 評価スクリプトは正常（Random vs Random で komi=0 なら約50%）

### 原因

学習時に盤面を常に「黒=+1, 白=-1」の絶対座標でモデルに渡していた。

白番のとき：
- Reward は winner * perspective で正しく変換されていた
- しかし Board は黒視点固定
- モデルは「自分の石が -1」という矛盾した状態で学習
- 結果、何も学習できず Loss が一様分布（5.88）付近で停滞

### 対策

perspective = 1.0 if idx % 2 == 0 else -1.0
current_board = current_board * perspective

これにより Board / Seq / Reward が整合する。

### 教訓

> **「Reward だけでなく、全ての入力が視点と整合しているか確認せよ」**
