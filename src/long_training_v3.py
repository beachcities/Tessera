"""
Tessera Long Training Script v3.0
==================================
10æ™‚é–“ä»¥ä¸Šã®è€ä¹…é‹ç”¨å‘ã‘å…¨è‡ªå‹•å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Design Philosophy:
- ã‚¿ã‚¤ãƒ«åŒ–: 100ã‚²ãƒ¼ãƒ  = 1ã‚¿ã‚¤ãƒ«ã€ã‚¿ã‚¤ãƒ«å˜ä½ã§ç®¡ç†
- 3å±¤è€ä¹…æ€§: ã‚³ãƒ¼ãƒ‰ / Docker / ãƒ›ã‚¹ãƒˆ
- æ–‡åŒ–ç¶™æ‰¿: LLMãŒè§£æã—ã‚„ã™ã„JSONLinesãƒ­ã‚°
- GPUãƒã‚¤ãƒ†ã‚£ãƒ–: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æœ€å„ªå…ˆ

Safety Features:
- 1ã‚²ãƒ¼ãƒ å˜ä½ã®ä¾‹å¤–ä¿è­·
- SSMç™ºæ•£æ™‚ã®è‡ªå‹•ãƒªã‚»ãƒƒãƒˆ
- OOMæ™‚ã®è‡ªå‹•ãƒãƒƒãƒã‚µã‚¤ã‚ºç¸®å°
- GPUæ¸©åº¦ç›£è¦–ï¼ˆ80â„ƒã§ä¸€æ™‚åœæ­¢ï¼‰
- NaN/Infæ¤œçŸ¥ã¨è‡ªå‹•å¾©æ—§
- ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œçŸ¥ã¨å®šæœŸGC
- ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã®è‡ªå‹•å†é–‹

Scalability:
- RTX 4070 (8GB): BATCH_SIZE=16
- RTX 3090 (24GB): BATCH_SIZE=64
- A100 (80GB): BATCH_SIZE=256

Usage:
    python3.10 src/long_training_v3.py [--resume checkpoint.pth]

Version: 3.0.0
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.optim as optim
import time
import datetime
import traceback
import gc
import argparse
import subprocess
from pathlib import Path
from dataclasses import dataclass, asdict, field
from typing import Optional, Tuple, List, Dict, Any

# srcãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from monitor import TesseraMonitor, defragment_gpu_memory
from gpu_go_engine import GPUGoEngine, PASS_TOKEN, VOCAB_SIZE
from model import MambaModel, MambaStateCapture


# ============================================================
# Configuration with Auto-Scaling
# ============================================================

@dataclass
class ScalableConfig:
    """VRAMå®¹é‡ã«å¿œã˜ã¦è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚‹è¨­å®š"""
    
    # === åŸºæœ¬è¨­å®š ===
    NUM_GAMES: int = 100000          # ç·ã‚²ãƒ¼ãƒ æ•°ï¼ˆ10æ™‚é–“ä»¥ä¸Šåˆ†ï¼‰
    MOVES_PER_GAME: int = 150        # 1ã‚²ãƒ¼ãƒ ã‚ãŸã‚Šã®æœ€å¤§æ‰‹æ•°
    SEQ_LEN: int = 64                # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
    
    # === ã‚¿ã‚¤ãƒ«è¨­å®š ===
    TILE_SIZE: int = 100             # 1ã‚¿ã‚¤ãƒ« = 100ã‚²ãƒ¼ãƒ 
    
    # === ãƒ¢ãƒ‡ãƒ«è¨­å®š ===
    D_MODEL: int = 256
    N_LAYERS: int = 4
    LEARNING_RATE: float = 1e-4
    WEIGHT_DECAY: float = 0.01
    
    # === ãƒ­ã‚°ãƒ»ä¿å­˜è¨­å®š ===
    LOG_INTERVAL: int = 10           # ãƒ­ã‚°å‡ºåŠ›é–“éš”ï¼ˆã‚²ãƒ¼ãƒ æ•°ï¼‰
    CHECKPOINT_INTERVAL: int = 500   # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé–“éš”ï¼ˆã‚²ãƒ¼ãƒ æ•°ï¼‰
    TILE_CHECKPOINT: bool = True     # ã‚¿ã‚¤ãƒ«ã”ã¨ã«ã‚‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
    
    # === å®‰å…¨è£…ç½®ã®é–¾å€¤ ===
    SSM_NORM_SOFT_LIMIT: float = 100.0   # è­¦å‘Šãƒ¬ãƒ™ãƒ«
    SSM_NORM_HARD_LIMIT: float = 300.0   # ãƒªã‚»ãƒƒãƒˆãƒ¬ãƒ™ãƒ«
    MAX_VRAM_RATIO: float = 0.90         # VRAMä½¿ç”¨ç‡ä¸Šé™
    GRADIENT_CLIP_NORM: float = 0.5      # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
    
    # === GPUæ¸©åº¦ç®¡ç† ===
    GPU_TEMP_WARNING: int = 75           # è­¦å‘Šæ¸©åº¦
    GPU_TEMP_THROTTLE: int = 80          # ã‚¹ãƒ­ãƒƒãƒˆãƒ«é–‹å§‹æ¸©åº¦
    GPU_TEMP_PAUSE: int = 85             # ä¸€æ™‚åœæ­¢æ¸©åº¦
    TEMP_CHECK_INTERVAL: int = 50        # æ¸©åº¦ãƒã‚§ãƒƒã‚¯é–“éš”ï¼ˆã‚²ãƒ¼ãƒ æ•°ï¼‰
    THROTTLE_SLEEP_SEC: float = 1.0      # ã‚¹ãƒ­ãƒƒãƒˆãƒ«æ™‚ã®å¾…æ©Ÿæ™‚é–“
    
    # === ãƒ¡ãƒ¢ãƒªç®¡ç† ===
    DEFRAG_INTERVAL: int = 50            # ãƒ‡ãƒ•ãƒ©ã‚°é–“éš”ï¼ˆæ‰‹æ•°ï¼‰
    FULL_GC_INTERVAL: int = 100          # å®Œå…¨GCé–“éš”ï¼ˆã‚²ãƒ¼ãƒ æ•°ï¼‰
    MEMORY_LEAK_THRESHOLD_MB: float = 500.0  # ãƒªãƒ¼ã‚¯æ¤œçŸ¥é–¾å€¤
    
    # === è‡ªå‹•ãƒªã‚«ãƒãƒª ===
    MAX_RETRIES_PER_GAME: int = 3        # 1ã‚²ãƒ¼ãƒ ã‚ãŸã‚Šã®æœ€å¤§ãƒªãƒˆãƒ©ã‚¤
    MAX_CONSECUTIVE_FAILURES: int = 10   # é€£ç¶šå¤±æ•—ã®ä¸Šé™
    RESET_INTERVAL: int = 1000           # å®šæœŸãƒªã‚»ãƒƒãƒˆé–“éš”ï¼ˆã‚²ãƒ¼ãƒ æ•°ï¼‰
    
    # === æ¸©åº¦ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚° ===
    TEMP_START: float = 1.5              # åˆæœŸæ¸©åº¦ï¼ˆæ¢ç´¢é‡è¦–ï¼‰
    TEMP_END: float = 0.8                # æœ€çµ‚æ¸©åº¦ï¼ˆåæŸé‡è¦–ï¼‰
    TEMP_DECAY_GAMES: int = 20000        # æ¸©åº¦æ¸›è¡°ã«è¦ã™ã‚‹ã‚²ãƒ¼ãƒ æ•°
    
    # === ãƒ‘ã‚¹ ===
    CHECKPOINT_DIR: str = "checkpoints"
    LOG_DIR: str = "logs"
    
    # === å‹•çš„ã«è¨­å®šã•ã‚Œã‚‹å€¤ ===
    BATCH_SIZE: int = 16
    
    @classmethod
    def auto_scale(cls, vram_gb: float) -> 'ScalableConfig':
        """VRAMå®¹é‡ã«å¿œã˜ã¦è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒ«"""
        config = cls()
        
        if vram_gb >= 70:      # A100 80GB
            config.BATCH_SIZE = 256
            config.D_MODEL = 512
            config.N_LAYERS = 8
            config.TILE_SIZE = 200
        elif vram_gb >= 40:    # A100 40GB / A6000
            config.BATCH_SIZE = 128
            config.D_MODEL = 384
            config.N_LAYERS = 6
            config.TILE_SIZE = 150
        elif vram_gb >= 20:    # RTX 3090 / 4090
            config.BATCH_SIZE = 64
            config.D_MODEL = 256
            config.N_LAYERS = 4
        elif vram_gb >= 10:    # RTX 3080
            config.BATCH_SIZE = 32
            config.D_MODEL = 256
            config.N_LAYERS = 4
        else:                  # RTX 4070 Laptop (8GB)
            config.BATCH_SIZE = 64
            config.D_MODEL = 256
            config.N_LAYERS = 4
        
        return config


# ============================================================
# Tile Data Structure
# ============================================================

@dataclass
class TileMetadata:
    """ã‚¿ã‚¤ãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆLLMç¶™æ‰¿ç”¨ï¼‰"""
    tile_id: int
    start_game: int
    end_game: int
    start_time: str
    end_time: str
    duration_sec: float
    games_played: int
    avg_loss: float
    min_loss: float
    max_loss: float
    total_moves: int
    resets: int
    errors: List[str] = field(default_factory=list)
    ssm_norm_avg: float = 0.0
    ssm_norm_max: float = 0.0
    vram_peak_mb: float = 0.0
    gpu_temp_max: int = 0
    status: str = "completed"


# ============================================================
# GPU Monitor
# ============================================================

class GPUMonitor:
    """GPUçŠ¶æ…‹ç›£è¦–"""
    
    @staticmethod
    def get_temperature() -> int:
        """GPUæ¸©åº¦ã‚’å–å¾—ï¼ˆâ„ƒï¼‰"""
        try:
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=temperature.gpu', '--format=csv,noheader,nounits'],
                capture_output=True, text=True, timeout=5
            )
            return int(result.stdout.strip().split('\n')[0])
        except:
            return 0  # å–å¾—å¤±æ•—æ™‚ã¯0ã‚’è¿”ã™
    
    @staticmethod
    def get_power_usage() -> float:
        """GPUæ¶ˆè²»é›»åŠ›ã‚’å–å¾—ï¼ˆWï¼‰"""
        try:
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits'],
                capture_output=True, text=True, timeout=5
            )
            return float(result.stdout.strip().split('\n')[0])
        except:
            return 0.0
    
    @staticmethod
    def get_memory_info() -> Dict[str, float]:
        """GPUãƒ¡ãƒ¢ãƒªæƒ…å ±ã‚’å–å¾—"""
        if not torch.cuda.is_available():
            return {'used_mb': 0, 'total_mb': 0, 'free_mb': 0}
        
        return {
            'used_mb': torch.cuda.memory_allocated() / 1024**2,
            'reserved_mb': torch.cuda.memory_reserved() / 1024**2,
            'total_mb': torch.cuda.get_device_properties(0).total_memory / 1024**2,
        }


# ============================================================
# Safety Manager
# ============================================================

class SafetyManager:
    """å®‰å…¨è£…ç½®ã®ç®¡ç†"""
    
    def __init__(self, config: ScalableConfig):
        self.config = config
        self.reset_count = 0
        self.consecutive_failures = 0
        self.last_reset_game = 0
        self.oom_count = 0
        self.original_batch_size = config.BATCH_SIZE
        self.initial_vram_mb = 0.0
        self.errors: List[str] = []
        
    def set_initial_vram(self):
        """åˆæœŸVRAMä½¿ç”¨é‡ã‚’è¨˜éŒ²"""
        mem = GPUMonitor.get_memory_info()
        self.initial_vram_mb = mem.get('used_mb', 0)
    
    def check_memory_leak(self) -> Tuple[bool, float]:
        """ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã‚’ãƒã‚§ãƒƒã‚¯"""
        mem = GPUMonitor.get_memory_info()
        current = mem.get('used_mb', 0)
        increase = current - self.initial_vram_mb
        
        if increase > self.config.MEMORY_LEAK_THRESHOLD_MB:
            return True, increase
        return False, increase
    
    def check_gpu_temperature(self) -> Tuple[str, int]:
        """GPUæ¸©åº¦ã‚’ãƒã‚§ãƒƒã‚¯"""
        temp = GPUMonitor.get_temperature()
        
        if temp >= self.config.GPU_TEMP_PAUSE:
            return "PAUSE", temp
        elif temp >= self.config.GPU_TEMP_THROTTLE:
            return "THROTTLE", temp
        elif temp >= self.config.GPU_TEMP_WARNING:
            return "WARNING", temp
        return "OK", temp
    
    def check_ssm_state(self, ssm_state: Optional[torch.Tensor]) -> Tuple[str, bool, float]:
        """SSMçŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯"""
        if ssm_state is None:
            return "OK", False, 0.0
        
        with torch.no_grad():
            norm = ssm_state.norm().item()
            
            if torch.isnan(ssm_state).any() or torch.isinf(ssm_state).any():
                return "NaN/Inf", True, norm
            
            if norm > self.config.SSM_NORM_HARD_LIMIT:
                return "DIVERGED", True, norm
            
            if norm > self.config.SSM_NORM_SOFT_LIMIT:
                return "WARNING", False, norm
        
        return "OK", False, norm
    
    def check_gradients(self, model: nn.Module) -> Tuple[str, bool]:
        """å‹¾é…ã‚’ãƒã‚§ãƒƒã‚¯"""
        total_norm = 0.0
        for p in model.parameters():
            if p.grad is not None:
                if torch.isnan(p.grad).any() or torch.isinf(p.grad).any():
                    return "NaN/Inf in gradients", True
                total_norm += p.grad.data.norm(2).item() ** 2
        
        total_norm = total_norm ** 0.5
        
        if total_norm > 100.0:
            return f"Gradient explosion ({total_norm:.1f})", True
        
        return "OK", False
    
    def check_loss(self, loss: float) -> Tuple[str, bool]:
        """Lossã‚’ãƒã‚§ãƒƒã‚¯"""
        import math
        if math.isnan(loss) or math.isinf(loss):
            return "NaN/Inf loss", True
        if loss > 20.0:
            return f"Abnormal loss ({loss:.1f})", True
        return "OK", False
    
    def record_success(self):
        """æˆåŠŸã‚’è¨˜éŒ²"""
        self.consecutive_failures = 0
    
    def record_failure(self, game_num: int, reason: str):
        """å¤±æ•—ã‚’è¨˜éŒ²"""
        self.consecutive_failures += 1
        self.errors.append(f"Game {game_num}: {reason}")
        if len(self.errors) > 100:
            self.errors.pop(0)
    
    def record_reset(self, game_num: int, reason: str):
        """ãƒªã‚»ãƒƒãƒˆã‚’è¨˜éŒ²"""
        self.reset_count += 1
        self.last_reset_game = game_num
    
    def should_abort(self) -> Tuple[bool, str]:
        """å®Œå…¨åœæ­¢ãŒå¿…è¦ã‹åˆ¤å®š"""
        if self.consecutive_failures >= self.config.MAX_CONSECUTIVE_FAILURES:
            return True, f"Too many consecutive failures ({self.consecutive_failures})"
        if self.oom_count >= 5:
            return True, f"Repeated OOM errors ({self.oom_count})"
        return False, ""
    
    def handle_oom(self) -> bool:
        """OOMç™ºç”Ÿæ™‚ã®å‡¦ç†ã€‚ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ç¸®å°"""
        self.oom_count += 1
        
        if self.config.BATCH_SIZE > 4:
            self.config.BATCH_SIZE = self.config.BATCH_SIZE // 2
            return True
        return False
    
    def get_temperature(self, game_num: int) -> float:
        """ç¾åœ¨ã®æ¸©åº¦ã‚’è¨ˆç®—ï¼ˆæ¢ç´¢â†’åæŸï¼‰"""
        progress = min(1.0, game_num / self.config.TEMP_DECAY_GAMES)
        temp = self.config.TEMP_START - (self.config.TEMP_START - self.config.TEMP_END) * progress
        return temp
    
    def get_recent_errors(self, n: int = 5) -> List[str]:
        """ç›´è¿‘ã®ã‚¨ãƒ©ãƒ¼ã‚’å–å¾—"""
        return self.errors[-n:]


# ============================================================
# Logger (JSON Lines + Console)
# ============================================================

class TesseraLogger:
    """ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ­ã‚°ï¼ˆJSONL + ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ï¼‰"""
    
    def __init__(self, log_dir: str):
        Path(log_dir).mkdir(exist_ok=True)
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        self.console_log = os.path.join(log_dir, f"training_v3_{timestamp}.log")
        self.jsonl_log = os.path.join(log_dir, f"training_v3_{timestamp}.jsonl")
        self.tile_log = os.path.join(log_dir, f"tiles_v3_{timestamp}.jsonl")
    
    def log(self, message: str, also_print: bool = True):
        """ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒ­ã‚°"""
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        line = f"[{timestamp}] {message}"
        
        if also_print:
            print(line, flush=True)
        
        with open(self.console_log, "a") as f:
            f.write(line + "\n")
    
    def log_json(self, event_type: str, data: Dict[str, Any]):
        """JSONLãƒ­ã‚°ï¼ˆLLMç¶™æ‰¿ç”¨ï¼‰"""
        entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "event": event_type,
            **data
        }
        with open(self.jsonl_log, "a") as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
    
    def log_tile(self, tile: TileMetadata):
        """ã‚¿ã‚¤ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°"""
        with open(self.tile_log, "a") as f:
            f.write(json.dumps(asdict(tile), ensure_ascii=False) + "\n")


# ============================================================
# Training Functions
# ============================================================

def save_checkpoint(model: nn.Module, 
                    optimizer: optim.Optimizer,
                    scheduler: Any,
                    game_num: int, 
                    stats: Dict,
                    config: ScalableConfig,
                    filepath: str):
    """å®Œå…¨ãªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
    torch.save({
        'game': game_num,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'stats': stats,
        'config': asdict(config) if hasattr(config, '__dataclass_fields__') else vars(config),
        'rng_state': torch.get_rng_state(),
        'cuda_rng_state': torch.cuda.get_rng_state() if torch.cuda.is_available() else None,
    }, filepath)


def load_checkpoint(filepath: str, 
                    model: nn.Module,
                    optimizer: optim.Optimizer,
                    scheduler: Any,
                    device: str) -> Tuple[int, Dict]:
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å¾©å…ƒ"""
    checkpoint = torch.load(filepath, map_location=device)
    
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    if scheduler and checkpoint.get('scheduler_state_dict'):
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    
    if checkpoint.get('rng_state') is not None:
        torch.set_rng_state(checkpoint['rng_state'])
    
    if checkpoint.get('cuda_rng_state') is not None and torch.cuda.is_available():
        torch.cuda.set_rng_state(checkpoint['cuda_rng_state'])
    
    return checkpoint['game'], checkpoint.get('stats', {})


def reset_training_state(model: MambaModel, 
                         optimizer: optim.Optimizer,
                         config: ScalableConfig,
                         logger: TesseraLogger):
    """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆé‡ã¿ã¯ä¿æŒï¼‰"""
    logger.log("ğŸ”„ Resetting training state (keeping weights)...")
    
    # Optimizerã®çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
    optimizer.state.clear()
    
    # GPUãƒ¡ãƒ¢ãƒªã‚’å®Œå…¨ã‚¯ãƒªã‚¢
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()


def run_single_game(engine: GPUGoEngine, 
                    model: MambaModel, 
                    state_capture: MambaStateCapture,
                    monitor: TesseraMonitor,
                    optimizer: optim.Optimizer,
                    criterion: nn.Module,
                    config: ScalableConfig,
                    safety: SafetyManager,
                    device: str,
                    game_num: int) -> Tuple[float, int, str, float]:
    """
    1ã‚²ãƒ¼ãƒ ã‚’å®Ÿè¡Œã—ã¦å­¦ç¿’
    
    Returns:
        (loss, moves, status, ssm_norm)
    """
    engine.reset()
    state_capture.clear()
    
    game_moves = []
    temperature = safety.get_temperature(game_num)
    ssm_norm_max = 0.0
    
    for move_num in range(config.MOVES_PER_GAME):
        try:
            # åˆæ³•æ‰‹å–å¾—
            legal_mask = engine.get_legal_mask()
            
            # ãƒ¢ãƒ‡ãƒ«æ¨è«–
            history = engine.get_current_sequence(max_len=config.SEQ_LEN)
            
            if history.shape[1] < config.SEQ_LEN:
                pad = torch.full(
                    (config.BATCH_SIZE, config.SEQ_LEN - history.shape[1]),
                    362, dtype=torch.long, device=device
                )
                seq = torch.cat([pad, history], dim=1)
            else:
                seq = history[:, -config.SEQ_LEN:]
            
            model.eval()
            with torch.no_grad():
                probs = model.get_move_probabilities(seq, legal_mask, temperature=temperature)
            
            if torch.isnan(probs).any():
                return 0.0, move_num, "NaN in probs", ssm_norm_max
            
            selected_moves = torch.multinomial(probs, num_samples=1).squeeze(-1)
            
            # ç€æ‰‹
            engine.play_batch(selected_moves)
            game_moves.append(selected_moves.clone())
            
            # çµ‚å±€ãƒã‚§ãƒƒã‚¯
            if engine.is_game_over().all():
                break
            
            # å®šæœŸçš„ã«SSMçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
            if move_num % 20 == 0:
                ssm_state = state_capture.get_last_state()
                status, needs_reset, norm = safety.check_ssm_state(ssm_state)
                ssm_norm_max = max(ssm_norm_max, norm)
                
                if needs_reset:
                    return 0.0, move_num, status, ssm_norm_max
            
            # å®šæœŸçš„ã«ãƒ‡ãƒ•ãƒ©ã‚°
            if move_num > 0 and move_num % config.DEFRAG_INTERVAL == 0:
                defragment_gpu_memory()
                
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                return 0.0, move_num, "OOM", ssm_norm_max
            raise
    
    # å­¦ç¿’
    loss_value = 0.0
    if len(game_moves) > 1:
        try:
            model.train()
            
            moves_tensor = torch.stack(game_moves, dim=1)
            
            if moves_tensor.shape[1] > 1:
                input_seq = moves_tensor[:, :-1]
                target_seq = moves_tensor[:, 1:]
                
                if input_seq.shape[1] < config.SEQ_LEN:
                    pad_len = config.SEQ_LEN - input_seq.shape[1]
                    pad = torch.full((config.BATCH_SIZE, pad_len), 362, dtype=torch.long, device=device)
                    input_seq = torch.cat([pad, input_seq], dim=1)
                    target_pad = torch.full((config.BATCH_SIZE, pad_len), 362, dtype=torch.long, device=device)
                    target_seq = torch.cat([target_pad, target_seq], dim=1)
                
                optimizer.zero_grad()
                logits = model(input_seq)
                
                logits = logits.contiguous()
                target_seq = target_seq.contiguous()
                
                loss = criterion(logits.reshape(-1, VOCAB_SIZE), target_seq.reshape(-1))
                
                status, needs_reset = safety.check_loss(loss.item())
                if needs_reset:
                    return 0.0, len(game_moves), status, ssm_norm_max
                
                loss.backward()
                
                status, needs_reset = safety.check_gradients(model)
                if needs_reset:
                    optimizer.zero_grad()
                    return 0.0, len(game_moves), status, ssm_norm_max
                
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.GRADIENT_CLIP_NORM)
                
                optimizer.step()
                
                loss_value = loss.item()
                
            # ãƒ†ãƒ³ã‚½ãƒ«ã‚’æ˜ç¤ºçš„ã«å‰Šé™¤
            del moves_tensor, input_seq, target_seq, logits
                
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                optimizer.zero_grad()
                defragment_gpu_memory()
                return 0.0, len(game_moves), "OOM", ssm_norm_max
            raise
    
    return loss_value, len(game_moves), "OK", ssm_norm_max


def main():
    """ãƒ¡ã‚¤ãƒ³ã®å­¦ç¿’ãƒ«ãƒ¼ãƒ—"""
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°
    parser = argparse.ArgumentParser()
    parser.add_argument('--resume', type=str, help='Resume from checkpoint')
    args = parser.parse_args()
    
    # GPUæƒ…å ±å–å¾—ã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    if torch.cuda.is_available():
        vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
        device = 'cuda'
    else:
        vram_gb = 0
        device = 'cpu'
    
    config = ScalableConfig.auto_scale(vram_gb)
    safety = SafetyManager(config)
    
    Path(config.CHECKPOINT_DIR).mkdir(exist_ok=True)
    logger = TesseraLogger(config.LOG_DIR)
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼
    logger.log("=" * 70)
    logger.log("ğŸš€ Tessera Long Training v3.0 - 10-Hour Endurance Mode")
    logger.log("=" * 70)
    
    logger.log(f"Device: {device}")
    if device == 'cuda':
        gpu_name = torch.cuda.get_device_name(0)
        logger.log(f"GPU: {gpu_name} ({vram_gb:.1f} GB)")
        temp_status, temp = safety.check_gpu_temperature()
        logger.log(f"GPU Temperature: {temp}Â°C ({temp_status})")
    
    logger.log(f"\nğŸ“Š Auto-Scaled Config:")
    logger.log(f"   Batch size: {config.BATCH_SIZE}")
    logger.log(f"   Model: d={config.D_MODEL}, layers={config.N_LAYERS}")
    logger.log(f"   Games: {config.NUM_GAMES}")
    logger.log(f"   Tile size: {config.TILE_SIZE} games")
    logger.log(f"   Temperature: {config.TEMP_START} â†’ {config.TEMP_END}")
    
    # åˆæœŸåŒ–
    logger.log("\nğŸ“¦ Initializing components...")
    
    engine = GPUGoEngine(batch_size=config.BATCH_SIZE, device=device)
    model = MambaModel(
        vocab_size=VOCAB_SIZE, 
        d_model=config.D_MODEL, 
        n_layers=config.N_LAYERS
    ).to(device)
    monitor = TesseraMonitor()
    state_capture = MambaStateCapture(model)
    
    optimizer = optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)
    criterion = nn.CrossEntropyLoss(ignore_index=362)
    
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=5000, T_mult=2, eta_min=1e-6
    )
    
    num_params = sum(p.numel() for p in model.parameters())
    logger.log(f"Model parameters: {num_params:,}")
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹
    start_game = 1
    stats = {
        'total_games': 0,
        'total_resets': 0,
        'best_loss': float('inf'),
        'total_moves': 0,
        'tiles_completed': 0,
    }
    
    if args.resume:
        logger.log(f"\nğŸ“‚ Resuming from {args.resume}...")
        start_game, stats = load_checkpoint(args.resume, model, optimizer, scheduler, device)
        start_game += 1
        logger.log(f"   Resumed at game {start_game}")
    
    # åˆæœŸVRAMè¨˜éŒ²
    safety.set_initial_vram()
    
    # JSONLãƒ­ã‚°ã«é–‹å§‹ã‚’è¨˜éŒ²
    logger.log_json("training_start", {
        "config": asdict(config),
        "gpu": torch.cuda.get_device_name(0) if device == 'cuda' else "CPU",
        "vram_gb": vram_gb,
        "resume_from": args.resume,
    })
    
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    logger.log("\nğŸ® Starting training loop...\n")
    
    start_time = time.time()
    total_loss = 0.0
    total_steps = 0
    losses_window = []
    game_num = start_game - 1
    
    # ã‚¿ã‚¤ãƒ«ç®¡ç†
    current_tile_id = (start_game - 1) // config.TILE_SIZE
    tile_start_time = time.time()
    tile_start_game = start_game
    tile_losses = []
    tile_moves = 0
    tile_resets = 0
    tile_ssm_norms = []
    tile_errors = []
    vram_peak = 0.0
    gpu_temp_max = 0
    
    try:
        for game_num in range(start_game, config.NUM_GAMES + 1):
            
            # === ã‚¿ã‚¤ãƒ«å¢ƒç•Œãƒã‚§ãƒƒã‚¯ ===
            new_tile_id = (game_num - 1) // config.TILE_SIZE
            if new_tile_id != current_tile_id and current_tile_id >= 0:
                # å‰ã®ã‚¿ã‚¤ãƒ«ã‚’å®Œäº†
                tile_end_time = time.time()
                tile = TileMetadata(
                    tile_id=current_tile_id,
                    start_game=tile_start_game,
                    end_game=game_num - 1,
                    start_time=datetime.datetime.fromtimestamp(tile_start_time).isoformat(),
                    end_time=datetime.datetime.fromtimestamp(tile_end_time).isoformat(),
                    duration_sec=tile_end_time - tile_start_time,
                    games_played=game_num - tile_start_game,
                    avg_loss=sum(tile_losses) / len(tile_losses) if tile_losses else 0,
                    min_loss=min(tile_losses) if tile_losses else 0,
                    max_loss=max(tile_losses) if tile_losses else 0,
                    total_moves=tile_moves,
                    resets=tile_resets,
                    errors=tile_errors[-5:],  # ç›´è¿‘5ã‚¨ãƒ©ãƒ¼
                    ssm_norm_avg=sum(tile_ssm_norms) / len(tile_ssm_norms) if tile_ssm_norms else 0,
                    ssm_norm_max=max(tile_ssm_norms) if tile_ssm_norms else 0,
                    vram_peak_mb=vram_peak,
                    gpu_temp_max=gpu_temp_max,
                )
                logger.log_tile(tile)
                logger.log(f"ğŸ“¦ Tile {current_tile_id} completed: {tile.games_played} games, avg_loss={tile.avg_loss:.4f}")
                
                # ã‚¿ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
                if config.TILE_CHECKPOINT:
                    ckpt_path = os.path.join(
                        config.CHECKPOINT_DIR,
                        f"tessera_tile{current_tile_id:05d}.pth"
                    )
                    save_checkpoint(model, optimizer, scheduler, game_num - 1, stats, config, ckpt_path)
                
                # æ–°ã‚¿ã‚¤ãƒ«é–‹å§‹
                current_tile_id = new_tile_id
                tile_start_time = time.time()
                tile_start_game = game_num
                tile_losses = []
                tile_moves = 0
                tile_resets = 0
                tile_ssm_norms = []
                tile_errors = []
                vram_peak = 0.0
                gpu_temp_max = 0
            
            # === GPUæ¸©åº¦ãƒã‚§ãƒƒã‚¯ ===
            if game_num % config.TEMP_CHECK_INTERVAL == 0:
                temp_status, temp = safety.check_gpu_temperature()
                gpu_temp_max = max(gpu_temp_max, temp)
                
                if temp_status == "PAUSE":
                    logger.log(f"ğŸŒ¡ï¸ GPU too hot ({temp}Â°C), pausing for 60s...")
                    time.sleep(60)
                elif temp_status == "THROTTLE":
                    logger.log(f"ğŸŒ¡ï¸ GPU warm ({temp}Â°C), throttling...")
                    time.sleep(config.THROTTLE_SLEEP_SEC)
            
            # === ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒã‚§ãƒƒã‚¯ ===
            if game_num % config.FULL_GC_INTERVAL == 0:
                has_leak, increase = safety.check_memory_leak()
                if has_leak:
                    logger.log(f"âš ï¸ Memory leak detected (+{increase:.0f}MB), forcing GC...")
                    gc.collect()
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    safety.set_initial_vram()
                
                # VRAMãƒ”ãƒ¼ã‚¯æ›´æ–°
                mem = GPUMonitor.get_memory_info()
                vram_peak = max(vram_peak, mem.get('used_mb', 0))
            
            # === å®šæœŸãƒªã‚»ãƒƒãƒˆ ===
            if game_num > 1 and game_num % config.RESET_INTERVAL == 0:
                logger.log(f"ğŸ”„ Scheduled reset at game {game_num}")
                reset_training_state(model, optimizer, config, logger)
                engine = GPUGoEngine(batch_size=config.BATCH_SIZE, device=device)
            
            # === ã‚²ãƒ¼ãƒ å®Ÿè¡Œï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰ ===
            for retry in range(config.MAX_RETRIES_PER_GAME):
                try:
                    loss, moves, status, ssm_norm = run_single_game(
                        engine, model, state_capture, monitor, optimizer, criterion,
                        config, safety, device, game_num
                    )
                    break
                except Exception as e:
                    if retry < config.MAX_RETRIES_PER_GAME - 1:
                        logger.log(f"âš ï¸ Game {game_num} error (retry {retry+1}): {str(e)[:50]}")
                        torch.cuda.empty_cache()
                        continue
                    else:
                        loss, moves, status, ssm_norm = 0.0, 0, f"Exception: {str(e)[:50]}", 0.0
            
            # === çµæœå‡¦ç† ===
            tile_ssm_norms.append(ssm_norm)
            
            if status != "OK":
                safety.record_failure(game_num, status)
                tile_errors.append(status)
                tile_resets += 1
                
                if status == "OOM":
                    if not safety.handle_oom():
                        logger.log("âŒ Cannot reduce batch size further")
                        break
                    logger.log(f"ğŸ“‰ Reduced batch size to {config.BATCH_SIZE}")
                    engine = GPUGoEngine(batch_size=config.BATCH_SIZE, device=device)
                else:
                    reset_training_state(model, optimizer, config, logger)
                    engine = GPUGoEngine(batch_size=config.BATCH_SIZE, device=device)
                
                should_abort, reason = safety.should_abort()
                if should_abort:
                    logger.log(f"âŒ Aborting: {reason}")
                    break
                
                continue
            
            # æˆåŠŸ
            safety.record_success()
            
            if loss > 0:
                total_loss += loss
                total_steps += 1
                losses_window.append(loss)
                tile_losses.append(loss)
                if len(losses_window) > 100:
                    losses_window.pop(0)
                
                if loss < stats['best_loss']:
                    stats['best_loss'] = loss
            
            tile_moves += moves
            stats['total_moves'] += moves
            
            scheduler.step()
            
            stats['total_games'] = game_num
            stats['total_resets'] = safety.reset_count
            
            # === å®šæœŸãƒ­ã‚° ===
            if game_num % config.LOG_INTERVAL == 0:
                avg_loss = total_loss / total_steps if total_steps > 0 else 0
                recent_loss = sum(losses_window) / len(losses_window) if losses_window else 0
                elapsed = time.time() - start_time
                games_per_hour = game_num / (elapsed / 3600) if elapsed > 0 else 0
                eta_hours = (config.NUM_GAMES - game_num) / games_per_hour if games_per_hour > 0 else 0
                
                mem = GPUMonitor.get_memory_info()
                vram_pct = mem['used_mb'] / mem['total_mb'] * 100 if mem['total_mb'] > 0 else 0
                
                current_lr = optimizer.param_groups[0]['lr']
                temp = safety.get_temperature(game_num)
                
                logger.log(
                    f"Game {game_num:6d}/{config.NUM_GAMES} | "
                    f"Loss: {recent_loss:.4f} (best: {stats['best_loss']:.4f}) | "
                    f"VRAM: {vram_pct:.0f}% | "
                    f"Temp: {temp:.2f} | "
                    f"Resets: {safety.reset_count} | "
                    f"ETA: {eta_hours:.1f}h"
                )
                
                # JSONLãƒ­ã‚°
                logger.log_json("progress", {
                    "game": game_num,
                    "loss": recent_loss,
                    "best_loss": stats['best_loss'],
                    "vram_pct": vram_pct,
                    "resets": safety.reset_count,
                    "eta_hours": eta_hours,
                })
            
            # === ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ ===
            if game_num % config.CHECKPOINT_INTERVAL == 0:
                avg_loss = total_loss / total_steps if total_steps > 0 else 0
                ckpt_path = os.path.join(
                    config.CHECKPOINT_DIR, 
                    f"tessera_v3_game{game_num:06d}_loss{avg_loss:.4f}.pth"
                )
                save_checkpoint(model, optimizer, scheduler, game_num, stats, config, ckpt_path)
                logger.log(f"ğŸ’¾ Checkpoint: {ckpt_path}")
            
            # === å®šæœŸGC ===
            if game_num % 10 == 0:
                defragment_gpu_memory()
    
    except KeyboardInterrupt:
        logger.log("\nâš ï¸ Training interrupted by user")
    
    except Exception as e:
        logger.log(f"\nâŒ Fatal error: {str(e)}")
        logger.log(traceback.format_exc())
    
    finally:
        # æœ€çµ‚ã‚¿ã‚¤ãƒ«ã‚’è¨˜éŒ²
        if tile_losses:
            tile_end_time = time.time()
            tile = TileMetadata(
                tile_id=current_tile_id,
                start_game=tile_start_game,
                end_game=game_num,
                start_time=datetime.datetime.fromtimestamp(tile_start_time).isoformat(),
                end_time=datetime.datetime.fromtimestamp(tile_end_time).isoformat(),
                duration_sec=tile_end_time - tile_start_time,
                games_played=game_num - tile_start_game + 1,
                avg_loss=sum(tile_losses) / len(tile_losses),
                min_loss=min(tile_losses),
                max_loss=max(tile_losses),
                total_moves=tile_moves,
                resets=tile_resets,
                errors=tile_errors[-5:],
                ssm_norm_avg=sum(tile_ssm_norms) / len(tile_ssm_norms) if tile_ssm_norms else 0,
                ssm_norm_max=max(tile_ssm_norms) if tile_ssm_norms else 0,
                vram_peak_mb=vram_peak,
                gpu_temp_max=gpu_temp_max,
                status="interrupted" if game_num < config.NUM_GAMES else "completed"
            )
            logger.log_tile(tile)
        
        # æœ€çµ‚ä¿å­˜
        elapsed = time.time() - start_time
        avg_loss = total_loss / total_steps if total_steps > 0 else 0
        
        final_ckpt = os.path.join(
            config.CHECKPOINT_DIR,
            f"tessera_v3_final_game{game_num}_loss{avg_loss:.4f}.pth"
        )
        save_checkpoint(model, optimizer, scheduler, game_num, stats, config, final_ckpt)
        
        # ã‚µãƒãƒªãƒ¼
        logger.log("\n" + "=" * 70)
        logger.log("ğŸ“Š Training Summary")
        logger.log("=" * 70)
        logger.log(f"   Games completed: {game_num}")
        logger.log(f"   Total time: {elapsed/60:.1f} min ({elapsed/3600:.2f} hours)")
        logger.log(f"   Average loss: {avg_loss:.4f}")
        logger.log(f"   Best loss: {stats['best_loss']:.4f}")
        logger.log(f"   Total moves: {stats['total_moves']:,}")
        logger.log(f"   Total resets: {safety.reset_count}")
        logger.log(f"   OOM events: {safety.oom_count}")
        logger.log(f"   Final batch size: {config.BATCH_SIZE}")
        logger.log(f"   Tiles completed: {current_tile_id + 1}")
        logger.log(f"   Final checkpoint: {final_ckpt}")
        
        # JSONLã«çµ‚äº†ã‚’è¨˜éŒ²
        logger.log_json("training_end", {
            "games_completed": game_num,
            "total_hours": elapsed / 3600,
            "avg_loss": avg_loss,
            "best_loss": stats['best_loss'],
            "total_resets": safety.reset_count,
            "tiles_completed": current_tile_id + 1,
        })
        
        logger.log("\nâœ… Training complete!")


if __name__ == "__main__":
    main()
