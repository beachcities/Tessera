
import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn import functional as F
from mamba_ssm import Mamba
from src.data import TextLoader
import time

# --- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š (A100ç”¨ã«èª¿æ•´) ---
batch_size = 64
block_size = 256    # æ–‡è„ˆã®é•·ã•
max_iters = 1000    # å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•° (ãƒ‡ãƒ¢ç”¨ãªã®ã§å°‘ãªã‚)
eval_interval = 100 # ãƒ­ã‚°å‡ºåŠ›é–“éš”
learning_rate = 3e-4
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# ãƒ¢ãƒ‡ãƒ«è¨­å®š (Smallãƒ¢ãƒ‡ãƒ«ç›¸å½“)
d_model = 256
d_state = 16
d_conv = 4
expand = 2

def main():
    print(f"ğŸš€ Mambaå­¦ç¿’é–‹å§‹ (Device: {device})")
    
    # 1. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æº–å‚™
    loader = TextLoader(block_size=block_size, batch_size=batch_size, device=device)
    vocab_size = loader.vocab_size
    print(f"ğŸ“š ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: Vocab size = {vocab_size}")

    # 2. ãƒ¢ãƒ‡ãƒ«å®šç¾©
    class MambaLM(nn.Module):
        def __init__(self):
            super().__init__()
            self.embedding = nn.Embedding(vocab_size, d_model)
            self.mamba = Mamba(
                d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand
            )
            self.lm_head = nn.Linear(d_model, vocab_size)

        def forward(self, idx):
            x = self.embedding(idx)
            x = self.mamba(x)
            logits = self.lm_head(x)
            return logits

        def generate(self, idx, max_new_tokens):
            # æ¨è«–(ç”Ÿæˆ)ãƒ¢ãƒ¼ãƒ‰
            for _ in range(max_new_tokens):
                logits = self(idx)
                logits = logits[:, -1, :] # æœ€å¾Œã®æ–‡å­—ã®äºˆæ¸¬ã ã‘ä½¿ã†
                probs = F.softmax(logits, dim=-1)
                idx_next = torch.multinomial(probs, num_samples=1)
                idx = torch.cat((idx, idx_next), dim=1)
            return idx

    model = MambaLM().to(device)
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)
    print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰å®Œäº†: å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã«å…¥ã‚Šã¾ã™...")

    # 3. å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    start_time = time.time()
    model.train()
    
    for iter in range(max_iters):
        # ãƒãƒƒãƒå–å¾—
        xb, yb = loader.get_batch('train')

        # é †ä¼æ’­ãƒ»é€†ä¼æ’­
        logits = model(xb)
        B, T, C = logits.shape
        loss = F.cross_entropy(logits.view(B*T, C), yb.view(B*T))
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # ãƒ­ã‚°å‡ºåŠ›
        if iter % eval_interval == 0:
            print(f"step {iter}: loss {loss.item():.4f}")

    end_time = time.time()
    print(f"âœ… å­¦ç¿’å®Œäº†ï¼ (æ‰€è¦æ™‚é–“: {end_time - start_time:.2f}ç§’)")

    # 4. ç”Ÿæˆãƒ‡ãƒ¢ (Inference)
    print("\nğŸ–‹ï¸ ç”Ÿæˆãƒ†ã‚¹ãƒˆ: MambaãŒæ›¸ãã‚·ã‚§ã‚¤ã‚¯ã‚¹ãƒ”ã‚¢...")
    print("-" * 50)
    
    context = torch.zeros((1, 1), dtype=torch.long, device=device) # 0ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆ
    generated_ids = model.generate(context, max_new_tokens=200)
    print(loader.decode(generated_ids[0].tolist()))
    
    print("-" * 50)

if __name__ == '__main__':
    main()
