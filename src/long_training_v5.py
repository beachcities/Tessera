"""
Tessera Long Training Script v4.1.0
====================================
ä¸¦åˆ—åŒ– + ELOè©•ä¾¡ + Graceful Shutdown çµ±åˆç‰ˆ
Clean Room Protocol å®Ÿè£…

Core Changes from v3:
- å„ãƒãƒƒãƒãŒç‹¬ç«‹ã—ãŸã‚²ãƒ¼ãƒ ã¨ã—ã¦é€²è¡Œ
- çµ‚å±€ã—ãŸç›¤é¢ã¯å³åº§ã«ãƒªã‚»ãƒƒãƒˆï¼†å­¦ç¿’
- ã‚¿ã‚¤ãƒ«æ¯ã«ELOè©•ä¾¡ã‚’å®Ÿè¡Œ
- æ­£ç¢ºãªGraceful Shutdown/Resume

Architecture:
- BATCH_SIZE = åŒæ™‚é€²è¡Œã‚²ãƒ¼ãƒ æ•°
- çµ‚å±€æ¤œçŸ¥ â†’ å­¦ç¿’ â†’ ãƒªã‚»ãƒƒãƒˆ â†’ ç¶™ç¶š
- SSMç™ºæ•£ãƒªã‚¹ã‚¯ã®ä½æ¸›ï¼ˆã‚²ãƒ¼ãƒ æ¯ã«ãƒªã‚»ãƒƒãƒˆï¼‰

Changelog:
- 4.1.0 (2026-01-13): Clean Room Protocol å®Ÿè£…
  - run_elo_evaluation ã§å­¦ç¿’ç”¨ãƒªã‚½ãƒ¼ã‚¹ã‚’ä¸€æ™‚è§£æ”¾
  - game_histories ã®æ˜ç¤ºçš„ã‚¯ãƒªã‚¢ï¼ˆã‚¾ãƒ³ãƒ“VRAMå¯¾ç­–ï¼‰
  - ã‚¨ãƒ³ã‚¸ãƒ³å†æ§‹ç¯‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
  - elo.py v1.3 ã¨ã®é€£æº
- 4.0.2 (2026-01-13): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã®ACDå¯¾å¿œ
  - A (Atomicity): ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«â†’renameã§ã‚¢ãƒˆãƒŸãƒƒã‚¯ä¿å­˜
  - C (Consistency): ä¿å­˜å¾Œã«èª­ã¿è¾¼ã¿æ¤œè¨¼
  - D (Durability): fsyncã§ç¢ºå®Ÿã«ãƒ‡ã‚£ã‚¹ã‚¯æ›¸ãè¾¼ã¿
- 4.0.1 (2026-01-13): ELOè©•ä¾¡æ™‚ã®ãƒ¡ãƒ¢ãƒªç®¡ç†å¼·åŒ–
- 4.0.0 (2026-01-13): åˆç‰ˆ

Version: 4.1.0
"""

import os
import sys
import json
import signal
import torch
import torch.nn as nn
import torch.optim as optim
import time
import datetime
import traceback
import gc
import argparse
from pathlib import Path
from dataclasses import dataclass, asdict, field
from typing import Optional, Tuple, List, Dict, Any

# srcãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from monitor import TesseraMonitor, defragment_gpu_memory
from gpu_go_engine import GPUGoEngine, PASS_TOKEN, VOCAB_SIZE, PAD_TOKEN, BOARD_SIZE
from tessera_model import TesseraModel
from model import MambaStateCapture  # äº’æ›ç”¨
from elo import ELOEvaluator, ELOLogger, TileELOTracker, ELOConfig, judge_games_by_stones


# ============================================================
# GPUGoEngine æ‹¡å¼µ: reset_selected
# ============================================================

def add_reset_selected_to_engine():
    """GPUGoEngineã«reset_selectedãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‹•çš„ã«è¿½åŠ """
    
    def reset_selected(self, mask: torch.Tensor):
        """
        æŒ‡å®šã•ã‚ŒãŸç›¤é¢ã®ã¿ãƒªã‚»ãƒƒãƒˆ
        
        Args:
            mask: (batch,) - True=ãƒªã‚»ãƒƒãƒˆå¯¾è±¡
        """
        if not mask.any():
            return
        
        indices = mask.nonzero().squeeze(-1)
        
        if indices.dim() == 0:
            indices = indices.unsqueeze(0)
        
        self.boards[indices] = 0
        self.turn[indices] = 0
        self.ko_point[indices] = -1
        self.last_move[indices] = -1
        self.consecutive_passes[indices] = 0
        self.move_count[indices] = 0
        self.history[indices] = PAD_TOKEN
    
    GPUGoEngine.reset_selected = reset_selected

# ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ 
add_reset_selected_to_engine()


# ============================================================
# Configuration
# ============================================================

@dataclass
class Config:
    """è¨­å®š"""
    
    # === åŸºæœ¬è¨­å®š ===
    NUM_GAMES: int = 1000000          # ç·ã‚²ãƒ¼ãƒ æ•°
    MAX_MOVES_PER_GAME: int = 200    # 1ã‚²ãƒ¼ãƒ ã‚ãŸã‚Šã®æœ€å¤§æ‰‹æ•°
    SEQ_LEN: int = 64                # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
    
    # === ãƒãƒƒãƒè¨­å®šï¼ˆä¸¦åˆ—ã‚²ãƒ¼ãƒ æ•°ï¼‰ ===
    BATCH_SIZE: int = 64             # åŒæ™‚é€²è¡Œã‚²ãƒ¼ãƒ æ•°
    
    # === ã‚¿ã‚¤ãƒ«è¨­å®š ===
    TILE_SIZE: int = 100             # 1ã‚¿ã‚¤ãƒ« = 100ã‚²ãƒ¼ãƒ 
    
    # === ãƒ¢ãƒ‡ãƒ«è¨­å®š ===
    D_MODEL: int = 256
    N_LAYERS: int = 4
    LEARNING_RATE: float = 1e-4
    WEIGHT_DECAY: float = 0.01
    GRADIENT_CLIP_NORM: float = 0.5
    
    # === ELOè¨­å®š ===
    ELO_EVAL_INTERVAL: int = 100     # ELOè©•ä¾¡é–“éš”ï¼ˆã‚²ãƒ¼ãƒ æ•°ï¼‰
    ELO_GAMES_PER_EVAL: int = 20     # è©•ä¾¡å¯¾æˆ¦æ•°
    
    # === ãƒ­ã‚°ãƒ»ä¿å­˜è¨­å®š ===
    LOG_INTERVAL: int = 50           # ãƒ­ã‚°å‡ºåŠ›é–“éš”ï¼ˆã‚²ãƒ¼ãƒ æ•°ï¼‰
    CHECKPOINT_INTERVAL: int = 500   # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé–“éš”
    
    # === å®‰å…¨è£…ç½® ===
    SSM_NORM_LIMIT: float = 300.0
    MAX_VRAM_RATIO: float = 0.90
    
    # === æ¸©åº¦ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚° ===
    TEMP_START: float = 1.5
    TEMP_END: float = 0.8
    TEMP_DECAY_GAMES: int = 20000
    
    # === ãƒ‘ã‚¹ ===
    CHECKPOINT_DIR: str = "checkpoints"
    LOG_DIR: str = "logs"
    
    @classmethod
    def auto_scale(cls, vram_gb: float) -> 'Config':
        """VRAMå®¹é‡ã«å¿œã˜ã¦è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒ«"""
        config = cls()
        
        if vram_gb >= 70:      # A100 80GB
            config.BATCH_SIZE = 256
            config.D_MODEL = 512
            config.N_LAYERS = 8
        elif vram_gb >= 40:    # A100 40GB
            config.BATCH_SIZE = 128
            config.D_MODEL = 384
            config.N_LAYERS = 6
        elif vram_gb >= 20:    # RTX 3090/4090
            config.BATCH_SIZE = 128
            config.D_MODEL = 256
            config.N_LAYERS = 4
        elif vram_gb >= 10:    # RTX 3080
            config.BATCH_SIZE = 128
            config.D_MODEL = 256
            config.N_LAYERS = 4
        else:                  # RTX 4070 Laptop (8GB)
            config.BATCH_SIZE = 128
            config.D_MODEL = 256
            config.N_LAYERS = 4
        
        return config


# ============================================================
# Graceful Shutdown Handler
# ============================================================

class GracefulShutdown:
    """Graceful Shutdown ãƒãƒ³ãƒ‰ãƒ©"""
    
    def __init__(self):
        self.shutdown_requested = False
        signal.signal(signal.SIGINT, self._handler)
        signal.signal(signal.SIGTERM, self._handler)
    
    def _handler(self, signum, frame):
        print("\nâš ï¸ Shutdown requested, finishing current batch...")
        self.shutdown_requested = True
    
    def should_stop(self) -> bool:
        return self.shutdown_requested


# ============================================================
# Logger
# ============================================================

class Logger:
    """ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ­ã‚°"""
    
    def __init__(self, log_dir: str):
        Path(log_dir).mkdir(exist_ok=True)
        timestamp = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9))).strftime("%Y%m%d_%H%M%S")
        
        self.console_log = os.path.join(log_dir, f"training_v4_{timestamp}.log")
        self.jsonl_log = os.path.join(log_dir, f"training_v4_{timestamp}.jsonl")
    
    def log(self, message: str, also_print: bool = True):
        timestamp = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9))).strftime("%Y-%m-%d %H:%M:%S")
        line = f"[{timestamp}] {message}"
        
        if also_print:
            print(line, flush=True)
        
        with open(self.console_log, "a") as f:
            f.write(line + "\n")
    
    def log_json(self, event_type: str, data: Dict[str, Any]):
        entry = {
            "timestamp": datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9))).isoformat(),
            "event": event_type,
            **data
        }
        with open(self.jsonl_log, "a") as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")


# ============================================================
# Checkpoint Functions (ACD: Atomic, Consistent, Durable)
# ============================================================

def save_checkpoint(model: nn.Module,
                    optimizer: optim.Optimizer,
                    scheduler,
                    stats: Dict,
                    config: Config,
                    filepath: str,
                    verify: bool = True):
    """
    ã‚¢ãƒˆãƒŸãƒƒã‚¯ + æ¤œè¨¼ + æ°¸ç¶šåŒ– ã«ã‚ˆã‚‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
    
    A (Atomicity): ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ« â†’ rename ã§ã‚¢ãƒˆãƒŸãƒƒã‚¯æ€§ã‚’æ‹…ä¿
    C (Consistency): ä¿å­˜å¾Œã«èª­ã¿è¾¼ã¿æ¤œè¨¼
    D (Durability): fsync ã§ç¢ºå®Ÿã«ãƒ‡ã‚£ã‚¹ã‚¯ã«æ›¸ãè¾¼ã¿
    
    Args:
        model: ãƒ¢ãƒ‡ãƒ«
        optimizer: ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
        scheduler: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
        stats: çµ±è¨ˆæƒ…å ±
        config: è¨­å®š
        filepath: ä¿å­˜å…ˆãƒ‘ã‚¹
        verify: ä¿å­˜å¾Œã«æ¤œè¨¼ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆTrueï¼‰
    """
    import tempfile
    import shutil
    
    dir_name = os.path.dirname(filepath) or '.'
    tmp_path = None
    
    try:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        fd, tmp_path = tempfile.mkstemp(dir=dir_name, suffix='.pth.tmp')
        os.close(fd)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        checkpoint_data = {
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
            'stats': stats,
            'config': asdict(config),
        }
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
        torch.save(checkpoint_data, tmp_path)
        
        # D (Durability): fsync ã§æ°¸ç¶šåŒ–
        with open(tmp_path, 'rb') as f:
            os.fsync(f.fileno())
        
        # A (Atomicity): ã‚¢ãƒˆãƒŸãƒƒã‚¯ã«ãƒªãƒãƒ¼ãƒ 
        shutil.move(tmp_path, filepath)
        tmp_path = None  # æˆåŠŸã—ãŸã®ã§ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸è¦
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚ fsyncï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ°¸ç¶šåŒ–ï¼‰
        try:
            dir_fd = os.open(dir_name, os.O_RDONLY)
            try:
                os.fsync(dir_fd)
            finally:
                os.close(dir_fd)
        except OSError:
            pass  # ä¸€éƒ¨ã®ç’°å¢ƒã§ã¯å¤±æ•—ã™ã‚‹ãŒè‡´å‘½çš„ã§ã¯ãªã„
        
        # C (Consistency): æ¤œè¨¼
        if verify:
            try:
                loaded = torch.load(filepath, map_location='cpu', weights_only=False)
                # ã‚­ãƒ¼ã®å­˜åœ¨ç¢ºèª
                assert 'model_state_dict' in loaded, "Missing model_state_dict"
                assert 'stats' in loaded, "Missing stats"
            except Exception as e:
                # æ¤œè¨¼å¤±æ•— â†’ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                if os.path.exists(filepath):
                    os.remove(filepath)
                raise RuntimeError(f"Checkpoint verification failed: {e}")
        
    except Exception as e:
        # å¤±æ•—ã—ãŸã‚‰ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if tmp_path and os.path.exists(tmp_path):
            try:
                os.remove(tmp_path)
            except OSError:
                pass
        raise RuntimeError(f"Checkpoint save failed: {e}")


def load_checkpoint(filepath: str,
                    model: nn.Module,
                    optimizer: optim.Optimizer,
                    scheduler,
                    device: str) -> Dict:
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å¾©å…ƒ"""
    checkpoint = torch.load(filepath, map_location=device, weights_only=False)
    
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    if scheduler and checkpoint.get('scheduler_state_dict'):
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    
    return checkpoint.get('stats', {})


# ============================================================
# Parallel Training Loop
# ============================================================

class ParallelTrainer:
    """ä¸¦åˆ—å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼"""
    
    def __init__(self,
                 config: Config,
                 device: str = 'cuda'):
        
        self.config = config
        self.device = device
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.engine = GPUGoEngine(batch_size=config.BATCH_SIZE, device=device)
        self.model = TesseraModel(
            vocab_size=VOCAB_SIZE,
            d_model=config.D_MODEL,
            n_layers=config.N_LAYERS
        ).to(device)
        
        
        # Phase III: æ®µéšçš„è§£å‡ï¼ˆProgressive Unfreezingï¼‰
        # Mamba ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨æ–°è¦å±¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆ†é›¢
        mamba_params = [p for n, p in self.model.named_parameters() if n.startswith('mamba.')]
        new_layer_params = [p for n, p in self.model.named_parameters() if not n.startswith('mamba.')]
        
        # Phase 3.1: Mamba ãƒ•ãƒªãƒ¼ã‚ºï¼ˆrequires_grad=Falseï¼‰
        for p in mamba_params:
            p.requires_grad = False
        
        self.optimizer = optim.AdamW([
            {'params': mamba_params, 'lr': 0.0},  # Phase 3.1 ã§ã¯å­¦ç¿’ã—ãªã„
            {'params': new_layer_params, 'lr': config.LEARNING_RATE}
        ], weight_decay=config.WEIGHT_DECAY)
        
        self.criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer, T_0=5000, T_mult=2, eta_min=1e-6
        )
        
        self.monitor = TesseraMonitor()
        #self.state_capture = MambaStateCapture(self.model)  # DISABLED: Memory leak
        
        # ELOè©•ä¾¡å™¨
        self.elo_evaluator = ELOEvaluator(
            engine_class=GPUGoEngine,
            model_class=TesseraModel,
            config=ELOConfig(games_per_evaluation=config.ELO_GAMES_PER_EVAL),
            checkpoint_dir=config.CHECKPOINT_DIR,
            device=device,
            model_kwargs={
                'vocab_size': VOCAB_SIZE,
                'd_model': config.D_MODEL,
                'n_layers': config.N_LAYERS
            }
        )
        self.elo_logger = ELOLogger(config.LOG_DIR)
        self.tile_elo_tracker = TileELOTracker(config.TILE_SIZE)
        
        # å„ã‚²ãƒ¼ãƒ ã®å±¥æ­´ã‚’ä¿æŒ
        self.game_histories: List[List[torch.Tensor]] = [[] for _ in range(config.BATCH_SIZE)]
        
        # çµ±è¨ˆ
        self.stats = {
            'total_games': 0,
            'total_moves': 0,
            'total_loss': 0.0,
            'loss_count': 0,
            'best_loss': float('inf'),
            'current_elo': 1500.0,
            'best_elo': 1500.0,
        }
        
        # å„ã‚²ãƒ¼ãƒ ã®æ‰‹æ•°ã‚’è¿½è·¡
        self.game_move_counts = torch.zeros(config.BATCH_SIZE, dtype=torch.long, device=device)
    
    def get_temperature(self, total_games: int) -> float:
        """æ¸©åº¦ã‚’å–å¾—"""
        progress = min(1.0, total_games / self.config.TEMP_DECAY_GAMES)
        return self.config.TEMP_START - (self.config.TEMP_START - self.config.TEMP_END) * progress
    
    def step(self) -> Tuple[int, float]:
        """
        1ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œï¼ˆå…¨ç›¤é¢ã«1æ‰‹ç€æ‰‹ï¼‰
        
        Returns:
            (finished_games, loss): çµ‚å±€ã—ãŸã‚²ãƒ¼ãƒ æ•°ã¨ãƒ­ã‚¹
        """
        # åˆæ³•æ‰‹ãƒã‚¹ã‚¯
        legal_mask = self.engine.get_legal_mask()
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æº–å‚™
        seq = self.engine.get_current_sequence(max_len=self.config.SEQ_LEN)
        if seq.shape[1] < self.config.SEQ_LEN:
            pad = torch.full(
                (self.config.BATCH_SIZE, self.config.SEQ_LEN - seq.shape[1]),
                PAD_TOKEN, dtype=torch.long, device=self.device
            )
            seq = torch.cat([pad, seq], dim=1)
        else:
            seq = seq[:, -self.config.SEQ_LEN:]
        
        # ãƒ¢ãƒ‡ãƒ«æ¨è«– - Phase III: ç›¤é¢ã‚‚æ¸¡ã™
        temp = self.get_temperature(self.stats['total_games'])
        
        # ç¾åœ¨ç›¤é¢ã‚’å–å¾— (engine.boards: [B, 2, 19, 19] -> [B, 19, 19])
        current_boards = self.engine.boards[:, 0] - self.engine.boards[:, 1]  # é»’=1, ç™½=-1, ç©º=0
        
        self.model.eval()
        with torch.no_grad():
            probs = self.model.get_move_probabilities(seq, current_boards, legal_mask, temperature=temp)
        # æ‰‹ã‚’é¸æŠ
        moves = torch.multinomial(probs, num_samples=1).squeeze(-1)
        
        # ç€æ‰‹
        self.engine.play_batch(moves)
        self.game_move_counts += 1
        
        # å±¥æ­´ã«è¿½åŠ 
        for i in range(self.config.BATCH_SIZE):
            self.game_histories[i].append(moves[i].clone())
        
        # çµ‚å±€æ¤œå‡º
        finished = self.engine.is_game_over()
        max_moves_reached = self.game_move_counts >= self.config.MAX_MOVES_PER_GAME
        should_end = finished | max_moves_reached
        
        # çµ‚å±€ã—ãŸã‚²ãƒ¼ãƒ ã®å‡¦ç†
        loss = 0.0
        num_finished = should_end.sum().item()
        
        if num_finished > 0:
            loss = self._process_finished_games(should_end)
        
        return num_finished, loss
    
    def _process_finished_games(self, finished_mask: torch.Tensor) -> float:
        """çµ‚å±€ã—ãŸã‚²ãƒ¼ãƒ ã‚’å‡¦ç†ï¼ˆå­¦ç¿’ï¼†ãƒªã‚»ãƒƒãƒˆï¼‰"""
        
        finished_indices = finished_mask.nonzero().squeeze(-1)
        if finished_indices.dim() == 0:
            finished_indices = finished_indices.unsqueeze(0)
        
        total_loss = 0.0
        num_learned = 0
        
        for idx in finished_indices:
            idx_item = idx.item()
            history = self.game_histories[idx_item]
            
            if len(history) > 1:
                # å­¦ç¿’
                loss = self._learn_from_game(history)
                total_loss += loss
                num_learned += 1
            
            # å±¥æ­´ã‚’ã‚¯ãƒªã‚¢
            self.game_histories[idx_item] = []
        
        # ã‚²ãƒ¼ãƒ æ‰‹æ•°ã‚’ãƒªã‚»ãƒƒãƒˆ
        self.game_move_counts[finished_mask] = 0
        
        # çµ‚å±€ã—ãŸç›¤é¢ã‚’ãƒªã‚»ãƒƒãƒˆ
        self.engine.reset_selected(finished_mask)
        
        # çµ±è¨ˆæ›´æ–°
        self.stats['total_games'] += len(finished_indices)
        
        return total_loss / num_learned if num_learned > 0 else 0.0
    
    def _learn_from_game(self, history: List[torch.Tensor]) -> float:
        """1ã‚²ãƒ¼ãƒ ã‹ã‚‰å­¦ç¿’ - Phase III å¯¾å¿œç‰ˆï¼ˆç›¤é¢å†æ§‹æˆï¼‰"""
        
        if len(history) < 2:
            return 0.0
        
        # å±¥æ­´ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«
        moves = torch.stack(history)  # (seq_len,)
        
        input_seq = moves[:-1].unsqueeze(0)  # (1, seq_len-1)
        target_seq = moves[1:].unsqueeze(0)  # (1, seq_len-1)
        
        # Phase III: ç›¤é¢ã‚’å†æ§‹æˆ
        with torch.no_grad():
            all_boards = self.engine.replay_history_to_boards_fast(moves[:-1])
            input_boards = all_boards.unsqueeze(0)  # (1, seq_len-1, 19, 19)
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        if input_seq.shape[1] < self.config.SEQ_LEN:
            pad_len = self.config.SEQ_LEN - input_seq.shape[1]
            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            pad = torch.full((1, pad_len), PAD_TOKEN, dtype=torch.long, device=self.device)
            input_seq = torch.cat([pad, input_seq], dim=1)
            target_pad = torch.full((1, pad_len), PAD_TOKEN, dtype=torch.long, device=self.device)
            target_seq = torch.cat([target_pad, target_seq], dim=1)
            # ç›¤é¢ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆç©ºç›¤é¢ã§åŸ‹ã‚ã‚‹ï¼‰
            board_pad = torch.zeros((1, pad_len, BOARD_SIZE, BOARD_SIZE), 
                                    dtype=torch.float32, device=self.device)
            input_boards = torch.cat([board_pad, input_boards], dim=1)
        
        # å­¦ç¿’
        self.model.train()
        self.optimizer.zero_grad()
        
        # Phase III: TesseraModel ã¯ (seq, board) ã‚’å—ã‘å–ã‚‹
        # æœ€å¾Œã®ç›¤é¢ã‚’ä½¿ç”¨ï¼ˆç¾åœ¨ã®ç›¤é¢çŠ¶æ…‹ï¼‰
        current_board = input_boards[:, -1, :, :]  # (1, 19, 19)
        logits, = self.model(input_seq, current_board, return_value=False)
        logits = logits.contiguous()
        target_seq = target_seq.contiguous()
        
        # Phase III: TesseraModel ã¯æœ€å¾Œã®ä¸€æ‰‹ã®ã¿äºˆæ¸¬
        # target_seq ã¯ [1, seq_len] ãªã®ã§ã€æœ€å¾Œã®æ‰‹ã ã‘ã‚’ä½¿ç”¨
        target_last = target_seq[:, -1]  # shape: [1]
        
        # logits ã¯ [1, VOCAB_SIZE] ãªã®ã§ reshape ä¸è¦
        loss = self.criterion(logits, target_last)
        
        if torch.isnan(loss) or torch.isinf(loss):
            return 0.0
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.GRADIENT_CLIP_NORM)
        self.optimizer.step()
        self.scheduler.step()
        
        loss_val = loss.item()
        
        # çµ±è¨ˆæ›´æ–°
        self.stats['total_loss'] += loss_val
        self.stats['loss_count'] += 1
        self.stats['total_moves'] += len(history)
        
        if loss_val < self.stats['best_loss']:
            self.stats['best_loss'] = loss_val
        
        return loss_val

    def run_elo_evaluation(self, model_name: str) -> Optional[Dict]:
        """
        ELOè©•ä¾¡ã‚’å®Ÿè¡Œï¼ˆClean Room Protocolï¼‰
        
        Contract:
            ã“ã®é–¢æ•°ã¯å­¦ç¿’ç”¨ãƒªã‚½ãƒ¼ã‚¹ï¼ˆEngine, Historyï¼‰ã‚’ä¸€æ™‚çš„ã«è§£æ”¾ã—ã€
            Clean Room ã§ ELO è©•ä¾¡ã‚’è¡Œã£ãŸå¾Œã€ãƒªã‚½ãƒ¼ã‚¹ã‚’å†æ§‹ç¯‰ã™ã‚‹ã€‚
            
        Pipeline:
            1. Release: å­¦ç¿’ç”¨ã‚¨ãƒ³ã‚¸ãƒ³ã¨å±¥æ­´ã‚’è§£æ”¾
            2. Sanitize: VRAMã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            3. Execute: ELOè©•ä¾¡ã‚’å®Ÿè¡Œ
            4. Rebuild: å­¦ç¿’ç”¨ãƒªã‚½ãƒ¼ã‚¹ã‚’å†æ§‹ç¯‰
        """
        import gc
        
        # === Step 1: Release Training Resources ===
        # å­¦ç¿’ç”¨ã‚¨ãƒ³ã‚¸ãƒ³ã‚’è§£æ”¾ï¼ˆVRAM ã®å¤§éƒ¨åˆ†ã‚’æ˜ã‘æ¸¡ã™ï¼‰
        if hasattr(self, 'engine') and self.engine is not None:
            del self.engine
            self.engine = None
        
        # Game History ã‚’ã‚¯ãƒªã‚¢ï¼ˆTensorå‚ç…§ã‚’åˆ‡ã‚‹ï¼‰
        # ã“ã‚Œã‚’å¿˜ã‚Œã‚‹ã¨ã€Œã‚¨ãƒ³ã‚¸ãƒ³ã‚’æ¶ˆã—ãŸã®ã«VRAMãŒç©ºã‹ãªã„ã€ã‚¾ãƒ³ãƒ“ç¾è±¡ãŒèµ·ãã‚‹
        for i in range(len(self.game_histories)):
            self.game_histories[i] = []
        
        # Game Move Counts ã‚’ãƒªã‚»ãƒƒãƒˆ
        self.game_move_counts = torch.zeros(
            self.config.BATCH_SIZE, dtype=torch.long, device=self.device
        )
        
        # === Step 2: Sanitize VRAM ===
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        # === Step 3: Execute ELO Evaluation ===
        self.model.eval()  # ELOè©•ä¾¡å‰ã« eval ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
        result = None
        try:
            # elo_evaluator.evaluate_and_update å†…éƒ¨ã§ VRAMSanitizer ãŒèµ°ã‚‹
            result = self.elo_evaluator.evaluate_and_update(self.model, model_name)
            
            if result:
                self.stats['current_elo'] = result['elo_after']
                if result['elo_after'] > self.stats['best_elo']:
                    self.stats['best_elo'] = result['elo_after']
                
                self.elo_logger.log_match(result)
                self.tile_elo_tracker.record_match(result, self.stats['total_games'])
        
        except Exception as e:
            print(f"âŒ ELO evaluation failed: {e}")
            import traceback
            traceback.print_exc()
        
        # === Step 4: Rebuild Training Resources ===
        # å­¦ç¿’ç”¨ã‚¨ãƒ³ã‚¸ãƒ³ã®å†æ§‹ç¯‰ï¼ˆã‚³ã‚¹ãƒˆ: ~100msã€å®‰å®šæ€§ã®ãŸã‚ã®å¿…è¦çµŒè²»ï¼‰
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        self.engine = GPUGoEngine(batch_size=self.config.BATCH_SIZE, device=self.device)
        
        # Game Histories ã‚’å†åˆæœŸåŒ–
        self.game_histories = [[] for _ in range(self.config.BATCH_SIZE)]
        
        return result


# ============================================================
# Main Function
# ============================================================

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--resume', type=str, help='Resume from checkpoint')
    args = parser.parse_args()
    
    # GPUæƒ…å ±
    if torch.cuda.is_available():
        vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
        device = 'cuda'
    else:
        vram_gb = 0
        device = 'cpu'
    
    config = Config.auto_scale(vram_gb)
    
    Path(config.CHECKPOINT_DIR).mkdir(exist_ok=True)
    Path(config.LOG_DIR).mkdir(exist_ok=True)
    
    logger = Logger(config.LOG_DIR)
    shutdown_handler = GracefulShutdown()
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼
    logger.log("=" * 70)
    logger.log("ğŸš€ Tessera Long Training v4.0 - Parallel + ELO")
    logger.log("=" * 70)
    logger.log(f"Device: {device}")
    
    if device == 'cuda':
        logger.log(f"GPU: {torch.cuda.get_device_name(0)} ({vram_gb:.1f} GB)")
    
    logger.log(f"\nğŸ“Š Config:")
    logger.log(f"   Batch size (parallel games): {config.BATCH_SIZE}")
    logger.log(f"   Model: d={config.D_MODEL}, layers={config.N_LAYERS}")
    logger.log(f"   Target games: {config.NUM_GAMES}")
    logger.log(f"   Tile size: {config.TILE_SIZE}")
    logger.log(f"   ELO eval interval: {config.ELO_EVAL_INTERVAL}")
    
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–
    logger.log("\nğŸ“¦ Initializing trainer...")
    trainer = ParallelTrainer(config, device)
    
    num_params = sum(p.numel() for p in trainer.model.parameters())
    logger.log(f"Model parameters: {num_params:,}")
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹
    if args.resume:
        logger.log(f"\nğŸ“‚ Resuming from {args.resume}...")
        trainer.stats = load_checkpoint(
            args.resume,
            trainer.model,
            trainer.optimizer,
            trainer.scheduler,
            device
        )
        logger.log(f"   Resumed at game {trainer.stats.get('total_games', 0)}")
    
    # JSON ãƒ­ã‚°ã«é–‹å§‹ã‚’è¨˜éŒ²
    logger.log_json("training_start", {
        "config": asdict(config),
        "vram_gb": vram_gb,
    })
    
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    logger.log("\nğŸ® Starting parallel training loop...\n")
    
    start_time = time.time()
    last_log_games = trainer.stats['total_games']
    last_elo_games = trainer.stats['total_games']
    last_checkpoint_games = trainer.stats['total_games']
    last_tile_games = trainer.stats['total_games']
    
    losses_window = trainer.stats.get('losses_window', [])
    
    try:
        while trainer.stats['total_games'] < config.NUM_GAMES:
            
            if shutdown_handler.should_stop():
                logger.log("ğŸ›‘ Graceful shutdown initiated...")
                break
            
            # 1ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ
            finished, loss = trainer.step()
            
            if loss > 0:
                losses_window.append(loss)
                if len(losses_window) > 100:
                    losses_window.pop(0)
            
            current_games = trainer.stats['total_games']
            
            # ãƒ­ã‚°å‡ºåŠ›
            if current_games - last_log_games >= config.LOG_INTERVAL:
                elapsed = time.time() - start_time
                games_per_hour = current_games / (elapsed / 3600) if elapsed > 0 else 0
                eta_hours = (config.NUM_GAMES - current_games) / games_per_hour if games_per_hour > 0 else 0
                
                avg_loss = trainer.stats['total_loss'] / trainer.stats['loss_count'] if trainer.stats['loss_count'] > 0 else 0
                recent_loss = sum(losses_window) / len(losses_window) if losses_window else 0
                
                logger.log(
                    f"Game {current_games:6d}/{config.NUM_GAMES} | "
                    f"Loss: {recent_loss:.4f} (best: {trainer.stats['best_loss']:.4f}) | "
                    f"ELO: {trainer.stats['current_elo']:.0f} | "
                    f"Speed: {games_per_hour:.0f}/hr | "
                    f"ETA: {eta_hours:.1f}h"
                )
                
                last_log_games = current_games
            
            # ELOè©•ä¾¡
            if current_games - last_elo_games >= config.ELO_EVAL_INTERVAL:
                model_name = f"game_{current_games:06d}"
                result = trainer.run_elo_evaluation(model_name)
                
                if result:
                    logger.log(
                        f"ğŸ“Š ELO: {result['elo_before']:.0f} â†’ {result['elo_after']:.0f} "
                        f"(vs {result['opponent_model']}, win_rate={result['win_rate']:.1%})"
                    )
                
                last_elo_games = current_games
            
            # ã‚¿ã‚¤ãƒ«å¢ƒç•Œ
            if current_games - last_tile_games >= config.TILE_SIZE:
                if trainer.tile_elo_tracker.tile_matches:
                    summary = trainer.tile_elo_tracker.close_tile(
                        current_games,
                        trainer.stats['current_elo']
                    )
                    trainer.elo_logger.log_tile_summary(asdict(summary))
                    logger.log(f"ğŸ“¦ Tile {summary.tile_id}: ELO {summary.elo_start:.0f}â†’{summary.elo_end:.0f}")
                
                last_tile_games = current_games
            
            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
            if current_games - last_checkpoint_games >= config.CHECKPOINT_INTERVAL:
                avg_loss = trainer.stats['total_loss'] / trainer.stats['loss_count'] if trainer.stats['loss_count'] > 0 else 0
                ckpt_path = os.path.join(
                    config.CHECKPOINT_DIR,
                    f"tessera_v4_game{current_games:06d}_elo{trainer.stats['current_elo']:.0f}.pth"
                )
                trainer.stats['losses_window'] = losses_window
                save_checkpoint(
                    trainer.model,
                    trainer.optimizer,
                    trainer.scheduler,
                    trainer.stats,
                    config,
                    ckpt_path
                )
                logger.log(f"ğŸ’¾ Checkpoint: {ckpt_path}")
                
                last_checkpoint_games = current_games
            
            # å®šæœŸçš„ã«ãƒ‡ãƒ•ãƒ©ã‚°
            if current_games % 100 == 0:
                defragment_gpu_memory()
    
    except Exception as e:
        logger.log(f"\nâŒ Error: {str(e)}")
        logger.log(traceback.format_exc())
    
    finally:
        # æœ€çµ‚ä¿å­˜
        elapsed = time.time() - start_time
        current_games = trainer.stats['total_games']
        avg_loss = trainer.stats['total_loss'] / trainer.stats['loss_count'] if trainer.stats['loss_count'] > 0 else 0
        
        final_ckpt = os.path.join(
            config.CHECKPOINT_DIR,
            f"tessera_v4_final_game{current_games}_elo{trainer.stats['current_elo']:.0f}.pth"
        )
        trainer.stats['losses_window'] = losses_window
        save_checkpoint(
            trainer.model,
            trainer.optimizer,
            trainer.scheduler,
            trainer.stats,
            config,
            final_ckpt
        )
        
        # ã‚µãƒãƒªãƒ¼
        logger.log("\n" + "=" * 70)
        logger.log("ğŸ“Š Training Summary")
        logger.log("=" * 70)
        logger.log(f"   Games completed: {current_games}")
        logger.log(f"   Total moves: {trainer.stats['total_moves']:,}")
        logger.log(f"   Total time: {elapsed/60:.1f} min ({elapsed/3600:.2f} hours)")
        logger.log(f"   Average loss: {avg_loss:.4f}")
        logger.log(f"   Best loss: {trainer.stats['best_loss']:.4f}")
        logger.log(f"   Final ELO: {trainer.stats['current_elo']:.0f}")
        logger.log(f"   Best ELO: {trainer.stats['best_elo']:.0f}")
        logger.log(f"   Final checkpoint: {final_ckpt}")
        
        logger.log_json("training_end", {
            "games_completed": current_games,
            "total_hours": elapsed / 3600,
            "avg_loss": avg_loss,
            "final_elo": trainer.stats['current_elo'],
        })
        
        logger.log("\nâœ… Training complete!")


if __name__ == "__main__":
    main()
